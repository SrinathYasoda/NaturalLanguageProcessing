{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tweepy\n",
    "import sqlite3\n",
    "from tweepy import Stream\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import API\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pickle\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sqlite3\n",
    "access_token=''\n",
    "access_token_secret=''\n",
    "consumer_key=''\n",
    "consumer_secret=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def createdatabase():\n",
    "    try:\n",
    "        conn=sqlite3.connect(\"Twitter_storage2.db\")\n",
    "        print(\"Database connection successfull\")\n",
    "        conn.execute('''CREATE TABLE search_twitter_data\n",
    "                (Date TEXT,\n",
    "                Text TEXT,\n",
    "                UserName TEXT,\n",
    "                ScreenName TEXT,\n",
    "                Bio TEXT,\n",
    "                FollowersCount INT,\n",
    "                FollowingCount INT,\n",
    "                Language TEXT,\n",
    "                RetweetCount INT,\n",
    "                Location TEXT,\n",
    "                Id_str TEXT,\n",
    "                Source TEXT,\n",
    "                Link TEXT);''')\n",
    "        print('Table created successfully')\n",
    "        conn.close()\n",
    "    except:\n",
    "        print('table already exists')\n",
    "    \n",
    "    return()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database connection successfull\n",
      "Table created successfully\n"
     ]
    }
   ],
   "source": [
    "createdatabase()\n",
    "conn=sqlite3.connect(\"Twitter_storage.db\")\n",
    "auth=OAuthHandler(consumer_key,consumer_secret)\n",
    "auth.set_access_token(access_token,access_token_secret)\n",
    "api=tweepy.API(auth,wait_on_rate_limit=True,wait_on_rate_limit_notify=True)\n",
    "\n",
    "#wait_on_rate_limit whether or not to automatically wait for rate limit to replenish\n",
    "#wait_on_rate_limit_notify whether or not to print a notification when Tweepy is waiting for rate limi to replenish\n",
    "#Sleep mode is automatically enabled with these two params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tweets...takes some time\n"
     ]
    }
   ],
   "source": [
    "tweetsPerQuery=100 #default\n",
    "maxTweets=10000 #for while loop\n",
    "\n",
    "#No sinceId and maxId \n",
    "sinceId=None\n",
    "maxId=-1\n",
    "tweet_count=0\n",
    "\n",
    "print('Downloading tweets...takes some time')\n",
    "\n",
    "searchquery='samsung'\n",
    "searchquery1=\"(samsung phone) OR (samsung smartphone) OR (samsung mobile) OR iphone OR motorola\\\n",
    "OR xiaomi OR redmi OR oppo OR oneplus OR (google pixel) OR (realme phone) OR (realme smartphone)\\\n",
    "OR (realme mobile) OR (huawei phone) OR (huawei smartphone) OR (huawei mobile) OR (huawei series)\\\n",
    "OR (samsung galaxy) OR (samsung series) OR (samsung note) OR (google nexus) OR (POCO smartphone)\\\n",
    "OR (poco mobile) OR (poco phone) OR (vivo phone) OR (vivo smartphone) OR (vivo mobile)\"\n",
    "\n",
    "#Knowledge\n",
    "#AND is the default e.g query=\"big data\" is \"big\" and \"data\" in the tweet.\n",
    "#OR is OR operator e.g.query=\"like\" OR \"love\"\n",
    "#Negation is done using \"-\" symbol e.g query=\"apple-phone\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully Completed\n"
     ]
    }
   ],
   "source": [
    "data=[]\n",
    "while (tweet_count<maxTweets):\n",
    "    conn=sqlite3.connect(\"Twitter_storage.db\")\n",
    "    try:\n",
    "        if(maxId<=0):\n",
    "            if(not sinceId):\n",
    "                new_tweets=api.search(q=searchquery1+\"-filter:retweets\",count=tweetsPerQuery,lang=\"en\",tweet_mode=\"extended\")\n",
    "            else:\n",
    "                new_tweets=api.search(q=searchquery1+\"-filter:retweets\",count=tweetsPerQuery,lang=\"en\",tweet_mode=\"extended\",sinceId=sinceId)\n",
    "        else:\n",
    "            if(not sinceId):\n",
    "                new_tweets=api.search(q=searchquery1+\"-filter:retweets\",count=tweetsPerQuery,lang=\"en\",tweet_mode=\"extended\",maxId=str(maxId-1))\n",
    "            else:\n",
    "                new_tweets=api.search(q=searchquery1+\"-filter:retweets\",count=tweetsPerQuery,lang=\"en\",tweet_mode=\"extended\",maxId=str(maxId-1),sinceId=sinceId)\n",
    "        \n",
    "        #No Tweets \n",
    "        if(not new_tweets):\n",
    "            break\n",
    "        \n",
    "        for tweet in new_tweets:\n",
    "            date=tweet.created_at\n",
    "            text=tweet.full_text            \n",
    "            username=tweet.user.name            \n",
    "            screenname=tweet.user.screen_name            \n",
    "            bio=tweet.user.description           \n",
    "            followerscount=tweet.user.followers_count\n",
    "            followingcount=tweet.user.friends_count           \n",
    "            language=tweet.lang\n",
    "            retweetcount=tweet.retweet_count\n",
    "            location=tweet.user.location            \n",
    "            id_str=tweet.id_str            \n",
    "            source=tweet.source            \n",
    "            #print(username)\n",
    "            link='https://twitter.com/'+str(''.join(screenname))+'/status/'+str(''.join(id_str))\n",
    "            \n",
    "            conn.execute(\"INSERT INTO search_twitter_data VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?)\",(date,text,username,screenname,bio,followerscount,followingcount,language,retweetcount,location,id_str,source,link))\n",
    "            conn.commit()\n",
    "            #Additional info f.write(jsonpickle.encode(tweet._json,unpicklable=False)+'\\n')\n",
    "            #(Date,Text,UserName,ScreenName,Bio,FollowersCount,FollowingCount,Language,RetweetCount,Location,Id_str,Source,Link)\n",
    "            #(str(date),str(text),str(username),str(screenname),str(bio),int(followerscount),int(followingcount),str(language),int(retweetcount),str(location),str(id_str),str(source),str(link))\n",
    "        tweet_count+=len(new_tweets)\n",
    "        maxId=new_tweets[-1].id\n",
    "        if tweet_count % 80000==0:\n",
    "            print(tweet_count)        \n",
    "        \n",
    "    except tweepy.TweepError as e:\n",
    "        print(\"Some error!!:\"+str(e))\n",
    "        continue\n",
    "conn.close()\n",
    "\n",
    "print(\"Successfully Completed\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn=sqlite3.connect(\"Twitter_storage.db\")\n",
    "conn.cursor()\n",
    "df=pd.read_sql_query(\"SELECT * from search_twitter_data\",conn)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date              1031\n",
       "Text              1031\n",
       "UserName          1031\n",
       "ScreenName        1031\n",
       "Bio               1031\n",
       "FollowersCount    1031\n",
       "FollowingCount    1031\n",
       "Language          1031\n",
       "RetweetCount      1031\n",
       "Location          1031\n",
       "Id_str            1031\n",
       "Source            1031\n",
       "Link              1031\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Text</th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Bio</th>\n",
       "      <th>FollowersCount</th>\n",
       "      <th>FollowingCount</th>\n",
       "      <th>Language</th>\n",
       "      <th>RetweetCount</th>\n",
       "      <th>Location</th>\n",
       "      <th>Id_str</th>\n",
       "      <th>Source</th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-11-12 04:01:40</td>\n",
       "      <td>RT @SuperSaf: NEW VIDEO: iPhone 12 Pro Max vs ...</td>\n",
       "      <td>planerbeat</td>\n",
       "      <td>planerbeat</td>\n",
       "      <td>–õ–ì–ë–¢-–∏–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä</td>\n",
       "      <td>28</td>\n",
       "      <td>111</td>\n",
       "      <td>en</td>\n",
       "      <td>43</td>\n",
       "      <td>–û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–µ –†–∞–±—Å–∫–∏–µ –í—ã–º–∏—Ä–∞—Ç—ã</td>\n",
       "      <td>1326736897792675841</td>\n",
       "      <td>Tweetbot for iŒüS</td>\n",
       "      <td>https://twitter.com/planerbeat/status/13267368...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-11-12 04:00:55</td>\n",
       "      <td>RT @SamsungMobile: Make their year, with Galax...</td>\n",
       "      <td>Victory‚Å∑BEüß®Dynamite</td>\n",
       "      <td>Khloe716</td>\n",
       "      <td>Î≥¥ÎùºÌï¥üíúBTS üíúÏïÑÎØ∏üíúüåà\\n'Teamwork Makes The Dream Work'...</td>\n",
       "      <td>58</td>\n",
       "      <td>215</td>\n",
       "      <td>en</td>\n",
       "      <td>2085</td>\n",
       "      <td></td>\n",
       "      <td>1326736708046483459</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>https://twitter.com/Khloe716/status/1326736708...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-11-12 04:00:41</td>\n",
       "      <td>RT @baejinnuna: Previously Hyunsuk mention in ...</td>\n",
       "      <td>BiiBoo ~ FIX</td>\n",
       "      <td>wana_santika</td>\n",
       "      <td>#CIX</td>\n",
       "      <td>75</td>\n",
       "      <td>314</td>\n",
       "      <td>en</td>\n",
       "      <td>48</td>\n",
       "      <td></td>\n",
       "      <td>1326736648718016512</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>https://twitter.com/wana_santika/status/132673...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-11-12 04:00:16</td>\n",
       "      <td>RT @yangwoning0209: #JUNGWON jungwonie bought ...</td>\n",
       "      <td>‚Ä¢0902‚Ä¢ ï‚Ä¢·¥•‚Ä¢ î</td>\n",
       "      <td>pkyjungwon</td>\n",
       "      <td>‡∏Ñ‡∏•‡∏±‡πà‡∏á‡∏£‡∏±‡∏Å ‡∏à‡∏≠‡∏á‡∏ß‡∏≠‡∏ô #Enhypen\\n@ENHYPEN_members\\n#E...</td>\n",
       "      <td>23</td>\n",
       "      <td>79</td>\n",
       "      <td>en</td>\n",
       "      <td>69</td>\n",
       "      <td></td>\n",
       "      <td>1326736542899924992</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>https://twitter.com/pkyjungwon/status/13267365...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-11-12 04:00:13</td>\n",
       "      <td>RT @baejinnuna: Previously Hyunsuk mention in ...</td>\n",
       "      <td>Mia Yales Oliverio üå∏</td>\n",
       "      <td>miaxxi0514</td>\n",
       "      <td>BYOUNGGON's GONISAUR üíô\\n\\nBOBBY's üíú\\n\\nFIX | i...</td>\n",
       "      <td>100</td>\n",
       "      <td>148</td>\n",
       "      <td>en</td>\n",
       "      <td>48</td>\n",
       "      <td>Philippines  Seoul</td>\n",
       "      <td>1326736532053483520</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>https://twitter.com/miaxxi0514/status/13267365...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2020-11-12 03:59:42</td>\n",
       "      <td>RT @yangwoning0209: #JUNGWON jungwonie bought ...</td>\n",
       "      <td>‚úú¬∞…™' ü ü ·¥°·¥Ä…™·¥õ “ì·¥è Ä  è·¥è·¥ú  è·¥è·¥è…¥…¢…™</td>\n",
       "      <td>BamgyuMin</td>\n",
       "      <td>·µê‚Å±À¢À¢‚Å±‚Åø·µç ·µó·µÉ·µè‚Å±·¥á·¥† Ä è·¥Ö è.\\n‚ù£Ô∏é‚ù£Ô∏é‚ù£Ô∏éñ†å</td>\n",
       "      <td>523</td>\n",
       "      <td>2494</td>\n",
       "      <td>en</td>\n",
       "      <td>69</td>\n",
       "      <td></td>\n",
       "      <td>1326736402831175680</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>https://twitter.com/BamgyuMin/status/132673640...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2020-11-12 03:59:10</td>\n",
       "      <td>RT @baejinnuna: Previously Hyunsuk mention in ...</td>\n",
       "      <td>an || À£¬π Reboot kajja!</td>\n",
       "      <td>myloveunsang</td>\n",
       "      <td>Multistan intinya mah saya teh suka banyak gru...</td>\n",
       "      <td>204</td>\n",
       "      <td>1331</td>\n",
       "      <td>en</td>\n",
       "      <td>48</td>\n",
       "      <td>üß°üíô</td>\n",
       "      <td>1326736265962688512</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>https://twitter.com/myloveunsang/status/132673...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2020-11-12 03:58:38</td>\n",
       "      <td>RT @gizbot: Samsung Galaxy F41 Is Ready To Rul...</td>\n",
       "      <td>Gizbot Telugu</td>\n",
       "      <td>GizbotTelugu</td>\n",
       "      <td>‡∞Æ‡±ä‡∞¨‡±à‡∞≤‡±ç ‡∞≤‡±Å ,‡∞≤‡∞æ‡∞™‡±ç ‡∞ü‡∞æ‡∞™‡±ç ‡∞≤‡±Å ,‡∞ü‡∞æ‡∞¨‡±ç‡∞≤‡±Ü‡∞ü‡±ç ,‡∞∏‡±ã‡∞∑‡∞≤‡±ç ‡∞®‡±Ü‡∞ü‡±ç‡∞µ...</td>\n",
       "      <td>698</td>\n",
       "      <td>15</td>\n",
       "      <td>en</td>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td>1326736134349742082</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>https://twitter.com/GizbotTelugu/status/132673...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2020-11-12 03:58:18</td>\n",
       "      <td>RT @yangwoning0209: #JUNGWON jungwonie bought ...</td>\n",
       "      <td>Sall üêë || üåæ</td>\n",
       "      <td>98_sbn</td>\n",
       "      <td>Day -19 üåÑ || You and I We Can Fly üïä..............</td>\n",
       "      <td>562</td>\n",
       "      <td>683</td>\n",
       "      <td>en</td>\n",
       "      <td>69</td>\n",
       "      <td></td>\n",
       "      <td>1326736050425786370</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>https://twitter.com/98_sbn/status/132673605042...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2020-11-12 03:56:50</td>\n",
       "      <td>RT @yangwoning0209: #JUNGWON jungwonie bought ...</td>\n",
       "      <td>ENGENE üêà</td>\n",
       "      <td>sheepinmoon</td>\n",
       "      <td>ENHYPEN ‚ù§Ô∏è ENGENE\\n.\\n7 or Nothing</td>\n",
       "      <td>108</td>\n",
       "      <td>166</td>\n",
       "      <td>en</td>\n",
       "      <td>69</td>\n",
       "      <td>üáÆüá©</td>\n",
       "      <td>1326735681771614210</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>https://twitter.com/sheepinmoon/status/1326735...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2020-11-12 03:55:53</td>\n",
       "      <td>RT @SamsungMobile: Make their year, with Galax...</td>\n",
       "      <td>ayamxxi‚Å∑</td>\n",
       "      <td>ayamkwon</td>\n",
       "      <td>BTS TXT ENHYPEN</td>\n",
       "      <td>161</td>\n",
       "      <td>432</td>\n",
       "      <td>en</td>\n",
       "      <td>2085</td>\n",
       "      <td></td>\n",
       "      <td>1326735440926289920</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>https://twitter.com/ayamkwon/status/1326735440...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2020-11-12 03:55:34</td>\n",
       "      <td>RT @LaxmikantWaghm7: Hurry Up !!!\\n\\nLast Day ...</td>\n",
       "      <td>AppleRetweetBot</td>\n",
       "      <td>AppleRTweet</td>\n",
       "      <td>Just a bot that retweets tweets containing #Ap...</td>\n",
       "      <td>836</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>From the Cloud</td>\n",
       "      <td>1326735360794251264</td>\n",
       "      <td></td>\n",
       "      <td>https://twitter.com/AppleRTweet/status/1326735...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2020-11-12 03:55:21</td>\n",
       "      <td>Hurry Up !!!\\n\\nLast Day Of Huge Discounts On ...</td>\n",
       "      <td>Laxmikant Waghmare</td>\n",
       "      <td>LaxmikantWaghm7</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>1326735306113093632</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>https://twitter.com/LaxmikantWaghm7/status/132...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2020-11-12 03:54:10</td>\n",
       "      <td>RT @taekinkook: [WTS] Malaysia Only [DM]\\n\\nBT...</td>\n",
       "      <td>byulbae</td>\n",
       "      <td>Shfqh404</td>\n",
       "      <td>my eyes look like I smoke weeds 24/7</td>\n",
       "      <td>193</td>\n",
       "      <td>141</td>\n",
       "      <td>en</td>\n",
       "      <td>9</td>\n",
       "      <td>my room</td>\n",
       "      <td>1326735008673902592</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>https://twitter.com/Shfqh404/status/1326735008...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2020-11-12 03:52:53</td>\n",
       "      <td>RT @philip_george70: A Samsung Galaxy j7 phone...</td>\n",
       "      <td>Moks</td>\n",
       "      <td>Santoscj_kings</td>\n",
       "      <td>Messi üêê, FC BARCELONA Fan‚öΩüíî, Roger Federer üéæ üêê...</td>\n",
       "      <td>7837</td>\n",
       "      <td>8527</td>\n",
       "      <td>en</td>\n",
       "      <td>65</td>\n",
       "      <td>The Unknown</td>\n",
       "      <td>1326734687126032385</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>https://twitter.com/Santoscj_kings/status/1326...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2020-11-12 03:52:51</td>\n",
       "      <td>RT @SamsungMobile: Make their year, with Galax...</td>\n",
       "      <td>Juguitopaupau</td>\n",
       "      <td>Juguitopaupau1</td>\n",
       "      <td>holaaa!!! nueva cuenta üôÇ soy JinnieZu4 üíú</td>\n",
       "      <td>8</td>\n",
       "      <td>95</td>\n",
       "      <td>en</td>\n",
       "      <td>2085</td>\n",
       "      <td></td>\n",
       "      <td>1326734679156908032</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>https://twitter.com/Juguitopaupau1/status/1326...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2020-11-12 03:52:16</td>\n",
       "      <td>@krunalpatelkp4 @MadhavSheth1 Jio4g is LTE+ ne...</td>\n",
       "      <td>nilesh katudia</td>\n",
       "      <td>nileshkatudia1</td>\n",
       "      <td></td>\n",
       "      <td>18</td>\n",
       "      <td>97</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>Mumbai, India</td>\n",
       "      <td>1326734531794104321</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>https://twitter.com/nileshkatudia1/status/1326...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2020-11-12 03:51:23</td>\n",
       "      <td>@RM_Update Jio4g is LTE+ network 100% but REAL...</td>\n",
       "      <td>nilesh katudia</td>\n",
       "      <td>nileshkatudia1</td>\n",
       "      <td></td>\n",
       "      <td>18</td>\n",
       "      <td>97</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>Mumbai, India</td>\n",
       "      <td>1326734307143004161</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>https://twitter.com/nileshkatudia1/status/1326...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2020-11-12 03:51:16</td>\n",
       "      <td>RT @baejinnuna: Previously Hyunsuk mention in ...</td>\n",
       "      <td>tina^</td>\n",
       "      <td>yongbaereal</td>\n",
       "      <td>he/she</td>\n",
       "      <td>44</td>\n",
       "      <td>148</td>\n",
       "      <td>en</td>\n",
       "      <td>48</td>\n",
       "      <td>WANNAONE/CIX/GOT7/STRAYKIDZ</td>\n",
       "      <td>1326734281192828928</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>https://twitter.com/yongbaereal/status/1326734...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2020-11-12 03:51:09</td>\n",
       "      <td>QC 3.0 Wall Charger, OKRAY 2 Pack 18W Fast Cha...</td>\n",
       "      <td>HotDeals</td>\n",
       "      <td>qefdchyt</td>\n",
       "      <td></td>\n",
       "      <td>103</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>1326734248850550784</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>https://twitter.com/qefdchyt/status/1326734248...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Date                                               Text  \\\n",
       "0   2020-11-12 04:01:40  RT @SuperSaf: NEW VIDEO: iPhone 12 Pro Max vs ...   \n",
       "1   2020-11-12 04:00:55  RT @SamsungMobile: Make their year, with Galax...   \n",
       "2   2020-11-12 04:00:41  RT @baejinnuna: Previously Hyunsuk mention in ...   \n",
       "3   2020-11-12 04:00:16  RT @yangwoning0209: #JUNGWON jungwonie bought ...   \n",
       "4   2020-11-12 04:00:13  RT @baejinnuna: Previously Hyunsuk mention in ...   \n",
       "5   2020-11-12 03:59:42  RT @yangwoning0209: #JUNGWON jungwonie bought ...   \n",
       "6   2020-11-12 03:59:10  RT @baejinnuna: Previously Hyunsuk mention in ...   \n",
       "7   2020-11-12 03:58:38  RT @gizbot: Samsung Galaxy F41 Is Ready To Rul...   \n",
       "8   2020-11-12 03:58:18  RT @yangwoning0209: #JUNGWON jungwonie bought ...   \n",
       "9   2020-11-12 03:56:50  RT @yangwoning0209: #JUNGWON jungwonie bought ...   \n",
       "10  2020-11-12 03:55:53  RT @SamsungMobile: Make their year, with Galax...   \n",
       "11  2020-11-12 03:55:34  RT @LaxmikantWaghm7: Hurry Up !!!\\n\\nLast Day ...   \n",
       "12  2020-11-12 03:55:21  Hurry Up !!!\\n\\nLast Day Of Huge Discounts On ...   \n",
       "13  2020-11-12 03:54:10  RT @taekinkook: [WTS] Malaysia Only [DM]\\n\\nBT...   \n",
       "14  2020-11-12 03:52:53  RT @philip_george70: A Samsung Galaxy j7 phone...   \n",
       "15  2020-11-12 03:52:51  RT @SamsungMobile: Make their year, with Galax...   \n",
       "16  2020-11-12 03:52:16  @krunalpatelkp4 @MadhavSheth1 Jio4g is LTE+ ne...   \n",
       "17  2020-11-12 03:51:23  @RM_Update Jio4g is LTE+ network 100% but REAL...   \n",
       "18  2020-11-12 03:51:16  RT @baejinnuna: Previously Hyunsuk mention in ...   \n",
       "19  2020-11-12 03:51:09  QC 3.0 Wall Charger, OKRAY 2 Pack 18W Fast Cha...   \n",
       "\n",
       "                      UserName       ScreenName  \\\n",
       "0                   planerbeat       planerbeat   \n",
       "1          Victory‚Å∑BEüß®Dynamite         Khloe716   \n",
       "2                 BiiBoo ~ FIX     wana_santika   \n",
       "3                  ‚Ä¢0902‚Ä¢ ï‚Ä¢·¥•‚Ä¢ î       pkyjungwon   \n",
       "4         Mia Yales Oliverio üå∏       miaxxi0514   \n",
       "5   ‚úú¬∞…™' ü ü ·¥°·¥Ä…™·¥õ “ì·¥è Ä  è·¥è·¥ú  è·¥è·¥è…¥…¢…™        BamgyuMin   \n",
       "6       an || À£¬π Reboot kajja!     myloveunsang   \n",
       "7                Gizbot Telugu     GizbotTelugu   \n",
       "8                  Sall üêë || üåæ           98_sbn   \n",
       "9                     ENGENE üêà      sheepinmoon   \n",
       "10                    ayamxxi‚Å∑         ayamkwon   \n",
       "11             AppleRetweetBot      AppleRTweet   \n",
       "12          Laxmikant Waghmare  LaxmikantWaghm7   \n",
       "13                     byulbae         Shfqh404   \n",
       "14                        Moks   Santoscj_kings   \n",
       "15               Juguitopaupau   Juguitopaupau1   \n",
       "16              nilesh katudia   nileshkatudia1   \n",
       "17              nilesh katudia   nileshkatudia1   \n",
       "18                       tina^      yongbaereal   \n",
       "19                    HotDeals         qefdchyt   \n",
       "\n",
       "                                                  Bio  FollowersCount  \\\n",
       "0                                     –õ–ì–ë–¢-–∏–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä              28   \n",
       "1   Î≥¥ÎùºÌï¥üíúBTS üíúÏïÑÎØ∏üíúüåà\\n'Teamwork Makes The Dream Work'...              58   \n",
       "2                                                #CIX              75   \n",
       "3   ‡∏Ñ‡∏•‡∏±‡πà‡∏á‡∏£‡∏±‡∏Å ‡∏à‡∏≠‡∏á‡∏ß‡∏≠‡∏ô #Enhypen\\n@ENHYPEN_members\\n#E...              23   \n",
       "4   BYOUNGGON's GONISAUR üíô\\n\\nBOBBY's üíú\\n\\nFIX | i...             100   \n",
       "5                        ·µê‚Å±À¢À¢‚Å±‚Åø·µç ·µó·µÉ·µè‚Å±·¥á·¥† Ä è·¥Ö è.\\n‚ù£Ô∏é‚ù£Ô∏é‚ù£Ô∏éñ†å             523   \n",
       "6   Multistan intinya mah saya teh suka banyak gru...             204   \n",
       "7   ‡∞Æ‡±ä‡∞¨‡±à‡∞≤‡±ç ‡∞≤‡±Å ,‡∞≤‡∞æ‡∞™‡±ç ‡∞ü‡∞æ‡∞™‡±ç ‡∞≤‡±Å ,‡∞ü‡∞æ‡∞¨‡±ç‡∞≤‡±Ü‡∞ü‡±ç ,‡∞∏‡±ã‡∞∑‡∞≤‡±ç ‡∞®‡±Ü‡∞ü‡±ç‡∞µ...             698   \n",
       "8   Day -19 üåÑ || You and I We Can Fly üïä..............             562   \n",
       "9                  ENHYPEN ‚ù§Ô∏è ENGENE\\n.\\n7 or Nothing             108   \n",
       "10                                    BTS TXT ENHYPEN             161   \n",
       "11  Just a bot that retweets tweets containing #Ap...             836   \n",
       "12                                                                  1   \n",
       "13               my eyes look like I smoke weeds 24/7             193   \n",
       "14  Messi üêê, FC BARCELONA Fan‚öΩüíî, Roger Federer üéæ üêê...            7837   \n",
       "15           holaaa!!! nueva cuenta üôÇ soy JinnieZu4 üíú               8   \n",
       "16                                                                 18   \n",
       "17                                                                 18   \n",
       "18                                             he/she              44   \n",
       "19                                                                103   \n",
       "\n",
       "    FollowingCount Language  RetweetCount                       Location  \\\n",
       "0              111       en            43  –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–µ –†–∞–±—Å–∫–∏–µ –í—ã–º–∏—Ä–∞—Ç—ã   \n",
       "1              215       en          2085                                  \n",
       "2              314       en            48                                  \n",
       "3               79       en            69                                  \n",
       "4              148       en            48             Philippines  Seoul   \n",
       "5             2494       en            69                                  \n",
       "6             1331       en            48                             üß°üíô   \n",
       "7               15       en             4                                  \n",
       "8              683       en            69                                  \n",
       "9              166       en            69                             üáÆüá©   \n",
       "10             432       en          2085                                  \n",
       "11               1       en             1                 From the Cloud   \n",
       "12               5       en             1                                  \n",
       "13             141       en             9                        my room   \n",
       "14            8527       en            65                    The Unknown   \n",
       "15              95       en          2085                                  \n",
       "16              97       en             0                  Mumbai, India   \n",
       "17              97       en             0                  Mumbai, India   \n",
       "18             148       en            48    WANNAONE/CIX/GOT7/STRAYKIDZ   \n",
       "19               0       en             0                                  \n",
       "\n",
       "                 Id_str               Source  \\\n",
       "0   1326736897792675841     Tweetbot for iŒüS   \n",
       "1   1326736708046483459  Twitter for Android   \n",
       "2   1326736648718016512  Twitter for Android   \n",
       "3   1326736542899924992  Twitter for Android   \n",
       "4   1326736532053483520   Twitter for iPhone   \n",
       "5   1326736402831175680  Twitter for Android   \n",
       "6   1326736265962688512  Twitter for Android   \n",
       "7   1326736134349742082      Twitter Web App   \n",
       "8   1326736050425786370  Twitter for Android   \n",
       "9   1326735681771614210  Twitter for Android   \n",
       "10  1326735440926289920  Twitter for Android   \n",
       "11  1326735360794251264                        \n",
       "12  1326735306113093632      Twitter Web App   \n",
       "13  1326735008673902592  Twitter for Android   \n",
       "14  1326734687126032385  Twitter for Android   \n",
       "15  1326734679156908032  Twitter for Android   \n",
       "16  1326734531794104321  Twitter for Android   \n",
       "17  1326734307143004161  Twitter for Android   \n",
       "18  1326734281192828928   Twitter for iPhone   \n",
       "19  1326734248850550784      Twitter Web App   \n",
       "\n",
       "                                                 Link  \n",
       "0   https://twitter.com/planerbeat/status/13267368...  \n",
       "1   https://twitter.com/Khloe716/status/1326736708...  \n",
       "2   https://twitter.com/wana_santika/status/132673...  \n",
       "3   https://twitter.com/pkyjungwon/status/13267365...  \n",
       "4   https://twitter.com/miaxxi0514/status/13267365...  \n",
       "5   https://twitter.com/BamgyuMin/status/132673640...  \n",
       "6   https://twitter.com/myloveunsang/status/132673...  \n",
       "7   https://twitter.com/GizbotTelugu/status/132673...  \n",
       "8   https://twitter.com/98_sbn/status/132673605042...  \n",
       "9   https://twitter.com/sheepinmoon/status/1326735...  \n",
       "10  https://twitter.com/ayamkwon/status/1326735440...  \n",
       "11  https://twitter.com/AppleRTweet/status/1326735...  \n",
       "12  https://twitter.com/LaxmikantWaghm7/status/132...  \n",
       "13  https://twitter.com/Shfqh404/status/1326735008...  \n",
       "14  https://twitter.com/Santoscj_kings/status/1326...  \n",
       "15  https://twitter.com/Juguitopaupau1/status/1326...  \n",
       "16  https://twitter.com/nileshkatudia1/status/1326...  \n",
       "17  https://twitter.com/nileshkatudia1/status/1326...  \n",
       "18  https://twitter.com/yongbaereal/status/1326734...  \n",
       "19  https://twitter.com/qefdchyt/status/1326734248...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'C:\\Users\\Srinath Yasoda\\JOBIFY\\ ClassNotes\\NLP\\Output_Backup.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Date                                               Text  \\\n",
      "0  12-11-2020 04:01  RT @SuperSaf: NEW VIDEO: iPhone 12 Pro Max vs ...   \n",
      "1  12-11-2020 04:00  RT @SamsungMobile: Make their year, with Galax...   \n",
      "2  12-11-2020 04:00  RT @baejinnuna: Previously Hyunsuk mention in ...   \n",
      "3  12-11-2020 04:00  RT @yangwoning0209: #JUNGWON jungwonie bought ...   \n",
      "4  12-11-2020 04:00  RT @baejinnuna: Previously Hyunsuk mention in ...   \n",
      "\n",
      "  Sentiment  \n",
      "0   Neutral  \n",
      "1  Positive  \n",
      "2  Positive  \n",
      "3   Neutral  \n",
      "4  Positive  \n",
      "Date         object\n",
      "Text         object\n",
      "Sentiment    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_fineTune=pd.read_csv('C:\\\\Users\\\\Srinath Yasoda\\\\JOBIFY\\\\ ClassNotes\\\\NLP\\\\Output.csv')\n",
    "df_ft=df_fineTune[['Date','Text','Sentiment']]\n",
    "print(df_ft.head())\n",
    "print(df_ft.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pickling the data\n",
    "#df_ft.to_pickle('C:\\\\Users\\\\Srinath Yasoda\\\\JOBIFY\\\\ ClassNotes\\\\NLP\\\\df_bckup.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cleanthedata(text):\n",
    "    text=(' '.join(re.sub(\"(RT @[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",str(text)).split()))   \n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-77-f28da7cd1902>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_ft['Text']=df_ft['Text'].apply(Cleanthedata)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "df_ft['Text']=df_ft['Text'].apply(Cleanthedata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1031, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12-11-2020 04:01</td>\n",
       "      <td>NEW VIDEO iPhone 12 Pro Max vs Samsung Galaxy ...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12-11-2020 04:00</td>\n",
       "      <td>Make their year with Galaxy Buds Live Learn more</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12-11-2020 04:00</td>\n",
       "      <td>Previously Hyunsuk mention in his previous Vli...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12-11-2020 04:00</td>\n",
       "      <td>JUNGWON jungwonie bought new phone it s samsun...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12-11-2020 04:00</td>\n",
       "      <td>Previously Hyunsuk mention in his previous Vli...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Date                                               Text  \\\n",
       "0  12-11-2020 04:01  NEW VIDEO iPhone 12 Pro Max vs Samsung Galaxy ...   \n",
       "1  12-11-2020 04:00   Make their year with Galaxy Buds Live Learn more   \n",
       "2  12-11-2020 04:00  Previously Hyunsuk mention in his previous Vli...   \n",
       "3  12-11-2020 04:00  JUNGWON jungwonie bought new phone it s samsun...   \n",
       "4  12-11-2020 04:00  Previously Hyunsuk mention in his previous Vli...   \n",
       "\n",
       "  Sentiment  \n",
       "0   Neutral  \n",
       "1  Positive  \n",
       "2  Positive  \n",
       "3   Neutral  \n",
       "4  Positive  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_ft.shape)\n",
    "df_ft.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Positive    180\n",
       "Spam        172\n",
       "Neutral     127\n",
       "Negative     49\n",
       "Neutal        1\n",
       " Spam         1\n",
       " Neutral      1\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ft['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date         531\n",
       "Text         531\n",
       "Sentiment    531\n",
       "dtype: int64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ft=df_ft.dropna()\n",
    "df_ft.isna().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "token=RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "cv=CountVectorizer(stop_words='english',ngram_range=(1,1),tokenizer=token.tokenize)\n",
    "textcounts=cv.fit_transform(df_ft['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X=textcounts\n",
    "Y=df_ft['Sentiment']\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNB accuracy score is 59.81%\n"
     ]
    }
   ],
   "source": [
    "#Bernouli--Features takes 2 values 0,1\n",
    "#Multinomial--Discrete data, eg:movie ratings, text data analysis(most repeated word)\n",
    "#Guassian--Because of assumptions of normal distributions GNB is used for continuous data\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "MNB=MultinomialNB()\n",
    "MNB.fit(X_train,Y_train)\n",
    "predict=MNB.predict(X_test)\n",
    "MAccuracy=accuracy_score(Y_test,predict)\n",
    "print('MNB accuracy score is {:04.2f}'.format(MAccuracy*100)+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to pickle and unpickle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6074766355140186\n",
      "0.6074766355140186\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#We use PICKLE to Serialize machine learning algorithms\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "MNB=MultinomialNB()\n",
    "MNB.fit(X_train,Y_train)\n",
    "\n",
    "#Saving the model using Pickle and Joblib\n",
    "\n",
    "#1) Using PICKLE\n",
    "filename='FinalizedModel.pkl'\n",
    "pickle.dump(MNB,open(filename,'wb'))\n",
    "\n",
    "#Load the pickled model\n",
    "loadmodel=pickle.load(open(filename,'rb'))\n",
    "result=loadmodel.score(X_test,Y_test)\n",
    "print(result)\n",
    "\n",
    "#2) Using Joblib\n",
    "FN='FinalisedModelJoblib.pkl'\n",
    "joblib.dump(MNB,FN)\n",
    "\n",
    "Load=joblib.load(FN)\n",
    "res=Load.score(X_test,Y_test)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNB2 accuracy score is 60.75%\n"
     ]
    }
   ],
   "source": [
    "## Using (2,2) ngrams\n",
    "cv=CountVectorizer(stop_words=\"english\",ngram_range=(2,2),tokenizer=token.tokenize)\n",
    "textcounts=cv.fit_transform(df_ft['Text'])\n",
    "X=textcounts\n",
    "Y=df_ft['Sentiment']\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,random_state=5)\n",
    "MNB=MultinomialNB()\n",
    "MNB.fit(X_train,Y_train)\n",
    "predict=MNB.predict(X_test)\n",
    "MAccuracy2=accuracy_score(predict,Y_test)\n",
    "print('MNB2 accuracy score is {:04.2f}'.format(MAccuracy2*100)+'%')\n",
    "=--------------------------\n",
    "## Using (3,3) ngrams\n",
    "cv=CountVectorizer(stop_words=\"english\",ngram_range=(3,3),tokenizer=token.tokenize)\n",
    "textcounts=cv.fit_transform(df_ft['Text'])\n",
    "X=textcounts\n",
    "Y=df_ft['Sentiment']\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,random_state=5)\n",
    "MNB=MultinomialNB()\n",
    "MNB.fit(X_train,Y_train)\n",
    "predict=MNB.predict(X_test)\n",
    "MAccuracy3=accuracy_score(predict,Y_test)\n",
    "print('MNB3 accuracy score is {:04.2f}'.format(MAccuracy3*100)+'%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNB3 accuracy score is 60.75%\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using (2,2) NGRAMS applying all Naive Bayes algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import ComplementNB\n",
    "#from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BNB accuracy = 62.62%\n",
      "CNB accuracy score is 63.55%\n"
     ]
    }
   ],
   "source": [
    "cv=CountVectorizer(stop_words=\"english\",ngram_range=(2,2),tokenizer=token.tokenize)\n",
    "textcounts=cv.fit_transform(df_ft['Text'])\n",
    "X=textcounts\n",
    "Y=df_ft['Sentiment']\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,random_state=5)\n",
    "\n",
    "#Bernoulli NB\n",
    "BNB = BernoulliNB()\n",
    "BNB.fit(X_train, Y_train)\n",
    "accuracy_score_bnb = accuracy_score(BNB.predict(X_test),Y_test)\n",
    "print('BNB accuracy = ' + str('{:4.2f}'.format(accuracy_score_bnb*100))+'%')\n",
    "\n",
    "\n",
    "#Complement NB\n",
    "CNB=ComplementNB()\n",
    "CNB.fit(X_train,Y_train)\n",
    "predict=CNB.predict(X_test)\n",
    "CNB_Accuracy_score=accuracy_score(predict,Y_test)\n",
    "print('CNB accuracy score is {:04.2f}'.format(CNB_Accuracy_score*100)+'%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TermFrequencyInverseDocumentFrequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf=TfidfVectorizer()\n",
    "texttoken2=tfidf.fit_transform(df_ft['Text'])\n",
    "\n",
    "X=texttoken2\n",
    "Y=df_ft['Sentiment']\n",
    "x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_state=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNB accuracy score tfidf is 62.62%\n",
      "CNB accuracy score is 63.55%\n",
      "BNB accuracy = 62.62%\n"
     ]
    }
   ],
   "source": [
    "#MNB \n",
    "MNB.fit(x_train,y_train)\n",
    "predict=MNB.predict(x_test)\n",
    "MNB_AccuracyScore=accuracy_score(predict,y_test)\n",
    "print('MNB accuracy score tfidf is {:04.2f}'.format(MNB_AccuracyScore*100)+'%')\n",
    "\n",
    "#CNB\n",
    "CNB.fit(x_train,y_train)\n",
    "predict=CNB.predict(x_test)\n",
    "CNB_Accuracy_score=accuracy_score(predict,y_test)\n",
    "print('CNB accuracy score is {:04.2f}'.format(CNB_Accuracy_score*100)+'%')\n",
    "\n",
    "#BNB\n",
    "BNB.fit(x_train, y_train)\n",
    "accuracy_score_bnb = accuracy_score(BNB.predict(x_test),y_test)\n",
    "print('BNB accuracy = ' + str('{:4.2f}'.format(accuracy_score_bnb*100))+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *---Complement Naive Bayes gave good accuracy for both CountVectorizer and Tfidf Vectorizer---*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling-- Latent Dirichlet Allocation(LDA) In Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dflda1=pd.read_csv('C:\\\\Users\\\\Srinath Yasoda\\\\JOBIFY\\\\ ClassNotes\\\\NLP\\\\Output.csv')\n",
    "dflda=dflda1[['Date','Text','Sentiment']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "dflda=dflda.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(531, 3)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Cleanthedata(text):\n",
    "    text=(' '.join(re.sub(\"(RT @[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",str(text)).split()))   \n",
    "    return text \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "dflda['Text']=dflda['Text'].apply(Cleanthedata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12-11-2020 04:01</td>\n",
       "      <td>NEW VIDEO iPhone 12 Pro Max vs Samsung Galaxy ...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12-11-2020 04:00</td>\n",
       "      <td>Make their year with Galaxy Buds Live Learn more</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12-11-2020 04:00</td>\n",
       "      <td>Previously Hyunsuk mention in his previous Vli...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12-11-2020 04:00</td>\n",
       "      <td>JUNGWON jungwonie bought new phone it s samsun...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12-11-2020 04:00</td>\n",
       "      <td>Previously Hyunsuk mention in his previous Vli...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Date                                               Text  \\\n",
       "0  12-11-2020 04:01  NEW VIDEO iPhone 12 Pro Max vs Samsung Galaxy ...   \n",
       "1  12-11-2020 04:00   Make their year with Galaxy Buds Live Learn more   \n",
       "2  12-11-2020 04:00  Previously Hyunsuk mention in his previous Vli...   \n",
       "3  12-11-2020 04:00  JUNGWON jungwonie bought new phone it s samsun...   \n",
       "4  12-11-2020 04:00  Previously Hyunsuk mention in his previous Vli...   \n",
       "\n",
       "  Sentiment  \n",
       "0   Neutral  \n",
       "1  Positive  \n",
       "2  Positive  \n",
       "3   Neutral  \n",
       "4  Positive  "
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dflda.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-203-b16ee0e4fd43>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_text['Index']=data_text.index\n"
     ]
    }
   ],
   "source": [
    "data_text=dflda[['Text']]\n",
    "data_text['Index']=data_text.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NEW VIDEO iPhone 12 Pro Max vs Samsung Galaxy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Make their year with Galaxy Buds Live Learn more</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Previously Hyunsuk mention in his previous Vli...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JUNGWON jungwonie bought new phone it s samsun...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Previously Hyunsuk mention in his previous Vli...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Index\n",
       "0  NEW VIDEO iPhone 12 Pro Max vs Samsung Galaxy ...      0\n",
       "1   Make their year with Galaxy Buds Live Learn more      1\n",
       "2  Previously Hyunsuk mention in his previous Vli...      2\n",
       "3  JUNGWON jungwonie bought new phone it s samsun...      3\n",
       "4  Previously Hyunsuk mention in his previous Vli...      4"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "# !pip install gensim\n",
    "import gensim\n",
    "import nltk\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS #for stopwords\n",
    "from nltk.stem import WordNetLemmatizer,SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original word</th>\n",
       "      <th>Stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>caresses</td>\n",
       "      <td>caress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>flies</td>\n",
       "      <td>fli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mules</td>\n",
       "      <td>mule</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>denied</td>\n",
       "      <td>deni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>agreed</td>\n",
       "      <td>agre</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Original word Stemmed\n",
       "0      caresses  caress\n",
       "1         flies     fli\n",
       "2         mules    mule\n",
       "3        denied    deni\n",
       "4        agreed    agre"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lemmatizing the words\n",
    "print(WordNetLemmatizer().lemmatize('went',pos='v'))\n",
    "\n",
    "#Stemmer example\n",
    "stemmer=SnowballStemmer('english')\n",
    "original_words=['caressessflies','mules','denied','agreed']\n",
    "\n",
    "abc=[stemmer.stem(plural) for plural in original_words]\n",
    "pd.DataFrame(data={'Original word':original_words,'Stemmed':abc})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining Stemming and Lemmatizing functions\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text,pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token)>3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    \n",
    "    return result\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  Text  Index\n",
      "531  Samsung Galaxy A01 A015M 16GB Dual SIM GSM Unl...    531\n"
     ]
    }
   ],
   "source": [
    "doc=data_text\n",
    "print(data_text[data_text['Index']==531])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Document:\n",
      "['Samsung', 'Galaxy', 'A01', 'A015M', '16GB', 'Dual', 'SIM', 'GSM', 'Unlocked', '5', '7', 'Display', 'Smartphone', 'International', 'Version', 'Red']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document\n",
      "['samsung', 'galaxi', 'dual', 'unlock', 'display', 'smartphon', 'intern', 'version']\n"
     ]
    }
   ],
   "source": [
    "doc_sample=doc[doc['Index']==531].values[0][0]\n",
    "print('Original Document:')\n",
    "words=[]\n",
    "for word in doc_sample.split(' '):\n",
    "    \n",
    "    words.append(word)\n",
    "print(words)\n",
    "print(\"\\n\\n tokenized and lemmatized document\")\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs=doc['Text'].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [video, iphon, samsung, galaxi, note, ultra, s...\n",
       "1                     [year, galaxi, bud, live, learn]\n",
       "2    [previous, hyunsuk, mention, previous, vlive, ...\n",
       "3    [jungwon, jungwoni, buy, phone, samsung, galax...\n",
       "4    [previous, hyunsuk, mention, previous, vlive, ...\n",
       "5    [jungwon, jungwoni, buy, phone, samsung, galax...\n",
       "6    [previous, hyunsuk, mention, previous, vlive, ...\n",
       "7    [samsung, galaxi, readi, rule, segment, notch,...\n",
       "8    [jungwon, jungwoni, buy, phone, samsung, galax...\n",
       "9    [jungwon, jungwoni, buy, phone, samsung, galax...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary=gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 camera\n",
      "1 comparison\n",
      "2 galaxi\n",
      "3 iphon\n",
      "4 note\n",
      "5 samsung\n",
      "6 supersafstyl\n",
      "7 ultra\n",
      "8 video\n",
      "9 bud\n",
      "10 learn\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for k,v in dictionary.iteritems():\n",
    "    print(k,v)\n",
    "    count+=1\n",
    "    if count>10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15,no_above=0.5,keep_n=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus=[dictionary.doc2bow(item) for item in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(21, 1)]"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus[530]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 21 (\"smartphon\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_corpus_530=bow_corpus[530]\n",
    "\n",
    "for i in range(len(bow_corpus_530)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_corpus_530[i][0],dictionary[bow_corpus_530[i][0]],bow_corpus_530[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "tfidf=models.TfidfModel(bow_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf=tfidf[bow_corpus]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.8553554162330351), (1, 0.5180416121517762)]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running LDA using Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model=gensim.models.LdaMulticore(bow_corpus,num_topics=10,id2word=dictionary,passes=2,workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic:0 \n",
      "Words: 0.426*\"smartphon\" + 0.178*\"oneplus\" + 0.110*\"mobil\" + 0.092*\"appl\" + 0.089*\"realm\" + 0.053*\"phone\" + 0.035*\"check\" + 0.004*\"case\" + 0.001*\"year\" + 0.001*\"member\"\n",
      "Topic:1 \n",
      "Words: 0.192*\"phone\" + 0.073*\"note\" + 0.073*\"jungwon\" + 0.073*\"member\" + 0.073*\"enhypen\" + 0.073*\"jungwoni\" + 0.073*\"buy\" + 0.066*\"smartphon\" + 0.058*\"iphon\" + 0.037*\"oneplus\"\n",
      "Topic:2 \n",
      "Words: 0.143*\"phone\" + 0.141*\"buy\" + 0.141*\"jungwoni\" + 0.141*\"enhypen\" + 0.141*\"jungwon\" + 0.141*\"note\" + 0.141*\"member\" + 0.001*\"iphon\" + 0.001*\"previous\" + 0.001*\"case\"\n",
      "Topic:3 \n",
      "Words: 0.364*\"iphon\" + 0.204*\"phone\" + 0.056*\"oneplus\" + 0.044*\"case\" + 0.044*\"compat\" + 0.044*\"note\" + 0.037*\"previous\" + 0.025*\"appl\" + 0.019*\"better\" + 0.019*\"hyunsuk\"\n",
      "Topic:4 \n",
      "Words: 0.163*\"phone\" + 0.132*\"note\" + 0.132*\"enhypen\" + 0.132*\"jungwon\" + 0.132*\"member\" + 0.132*\"buy\" + 0.132*\"jungwoni\" + 0.024*\"case\" + 0.004*\"previous\" + 0.003*\"check\"\n",
      "Topic:5 \n",
      "Words: 0.130*\"phone\" + 0.121*\"note\" + 0.121*\"buy\" + 0.121*\"jungwoni\" + 0.121*\"member\" + 0.121*\"jungwon\" + 0.121*\"enhypen\" + 0.066*\"smartphon\" + 0.020*\"oneplus\" + 0.015*\"mobil\"\n",
      "Topic:6 \n",
      "Words: 0.134*\"phone\" + 0.129*\"member\" + 0.129*\"jungwoni\" + 0.129*\"note\" + 0.129*\"jungwon\" + 0.129*\"enhypen\" + 0.129*\"buy\" + 0.012*\"oneplus\" + 0.009*\"check\" + 0.006*\"bud\"\n",
      "Topic:7 \n",
      "Words: 0.246*\"previous\" + 0.130*\"iphon\" + 0.117*\"yesterday\" + 0.117*\"mention\" + 0.117*\"hyunsuk\" + 0.117*\"better\" + 0.117*\"vlive\" + 0.011*\"case\" + 0.009*\"phone\" + 0.002*\"bud\"\n",
      "Topic:8 \n",
      "Words: 0.242*\"bud\" + 0.236*\"live\" + 0.236*\"learn\" + 0.236*\"year\" + 0.006*\"check\" + 0.006*\"compat\" + 0.006*\"malaysia\" + 0.006*\"sad\" + 0.006*\"photocard\" + 0.006*\"devic\"\n",
      "Topic:9 \n",
      "Words: 0.137*\"bud\" + 0.129*\"check\" + 0.119*\"compat\" + 0.119*\"photocard\" + 0.119*\"devic\" + 0.119*\"malaysia\" + 0.119*\"sad\" + 0.013*\"jungwon\" + 0.013*\"jungwoni\" + 0.013*\"enhypen\"\n"
     ]
    }
   ],
   "source": [
    "for idx,topic in lda_model.print_topics(-1):\n",
    "    print('Topic:{} \\nWords: {}'.format(idx,topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running LDA using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_tfidf=gensim.models.LdaMulticore(corpus_tfidf,num_topics=10,id2word=dictionary,passes=2,workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic:0 \n",
      "Words:0.141*\"case\" + 0.113*\"previous\" + 0.108*\"iphon\" + 0.066*\"phone\" + 0.060*\"yesterday\" + 0.060*\"hyunsuk\" + 0.060*\"better\" + 0.060*\"vlive\" + 0.060*\"mention\" + 0.026*\"smartphon\"\n",
      "Topic:1 \n",
      "Words:0.141*\"jungwoni\" + 0.141*\"enhypen\" + 0.141*\"buy\" + 0.141*\"member\" + 0.141*\"jungwon\" + 0.137*\"note\" + 0.112*\"phone\" + 0.008*\"live\" + 0.008*\"learn\" + 0.008*\"year\"\n",
      "Topic:2 \n",
      "Words:0.251*\"year\" + 0.251*\"learn\" + 0.251*\"live\" + 0.194*\"bud\" + 0.009*\"smartphon\" + 0.006*\"oneplus\" + 0.006*\"check\" + 0.006*\"appl\" + 0.001*\"devic\" + 0.001*\"sad\"\n",
      "Topic:3 \n",
      "Words:0.080*\"buy\" + 0.080*\"jungwon\" + 0.080*\"jungwoni\" + 0.080*\"member\" + 0.080*\"enhypen\" + 0.077*\"note\" + 0.066*\"phone\" + 0.037*\"compat\" + 0.033*\"bud\" + 0.031*\"iphon\"\n",
      "Topic:4 \n",
      "Words:0.215*\"year\" + 0.215*\"live\" + 0.215*\"learn\" + 0.172*\"bud\" + 0.014*\"devic\" + 0.014*\"malaysia\" + 0.014*\"sad\" + 0.014*\"photocard\" + 0.014*\"compat\" + 0.013*\"check\"\n",
      "Topic:5 \n",
      "Words:0.074*\"phone\" + 0.070*\"note\" + 0.067*\"jungwon\" + 0.067*\"member\" + 0.067*\"enhypen\" + 0.067*\"jungwoni\" + 0.067*\"buy\" + 0.042*\"iphon\" + 0.040*\"previous\" + 0.038*\"malaysia\"\n",
      "Topic:6 \n",
      "Words:0.156*\"compat\" + 0.124*\"photocard\" + 0.124*\"malaysia\" + 0.124*\"sad\" + 0.124*\"devic\" + 0.113*\"check\" + 0.072*\"bud\" + 0.042*\"iphon\" + 0.018*\"case\" + 0.015*\"learn\"\n",
      "Topic:7 \n",
      "Words:0.149*\"iphon\" + 0.084*\"year\" + 0.084*\"live\" + 0.084*\"learn\" + 0.080*\"note\" + 0.077*\"oneplus\" + 0.067*\"bud\" + 0.047*\"phone\" + 0.042*\"member\" + 0.042*\"buy\"\n",
      "Topic:8 \n",
      "Words:0.135*\"previous\" + 0.112*\"iphon\" + 0.108*\"phone\" + 0.062*\"note\" + 0.057*\"enhypen\" + 0.057*\"buy\" + 0.057*\"jungwon\" + 0.057*\"jungwoni\" + 0.057*\"member\" + 0.054*\"yesterday\"\n",
      "Topic:9 \n",
      "Words:0.250*\"smartphon\" + 0.131*\"oneplus\" + 0.100*\"appl\" + 0.097*\"check\" + 0.071*\"mobil\" + 0.049*\"realm\" + 0.037*\"iphon\" + 0.027*\"phone\" + 0.022*\"photocard\" + 0.022*\"sad\"\n"
     ]
    }
   ],
   "source": [
    "for idx,topic in lda_model_tfidf.print_topics(-1):\n",
    "    print(\"Topic:{} \\nWords:{}\".format(idx,topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification of the topics\n",
    "## 1.Performance evalution by classifying sample document using LDA Bag of words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jungwon',\n",
       " 'jungwoni',\n",
       " 'buy',\n",
       " 'phone',\n",
       " 'samsung',\n",
       " 'galaxi',\n",
       " 'note',\n",
       " 'enhypen',\n",
       " 'member']"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[530]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.5499924421310425\t \n",
      "Topic: 0.426*\"smartphon\" + 0.178*\"oneplus\" + 0.110*\"mobil\" + 0.092*\"appl\" + 0.089*\"realm\" + 0.053*\"phone\" + 0.035*\"check\" + 0.004*\"case\" + 0.001*\"year\" + 0.001*\"member\"\n",
      "\n",
      "Score: 0.0500035434961319\t \n",
      "Topic: 0.192*\"phone\" + 0.073*\"note\" + 0.073*\"jungwon\" + 0.073*\"member\" + 0.073*\"enhypen\" + 0.073*\"jungwoni\" + 0.073*\"buy\" + 0.066*\"smartphon\" + 0.058*\"iphon\" + 0.037*\"oneplus\"\n",
      "\n",
      "Score: 0.050003532320261\t \n",
      "Topic: 0.130*\"phone\" + 0.121*\"note\" + 0.121*\"buy\" + 0.121*\"jungwoni\" + 0.121*\"member\" + 0.121*\"jungwon\" + 0.121*\"enhypen\" + 0.066*\"smartphon\" + 0.020*\"oneplus\" + 0.015*\"mobil\"\n",
      "\n",
      "Score: 0.05000017583370209\t \n",
      "Topic: 0.134*\"phone\" + 0.129*\"member\" + 0.129*\"jungwoni\" + 0.129*\"note\" + 0.129*\"jungwon\" + 0.129*\"enhypen\" + 0.129*\"buy\" + 0.012*\"oneplus\" + 0.009*\"check\" + 0.006*\"bud\"\n",
      "\n",
      "Score: 0.05000017583370209\t \n",
      "Topic: 0.242*\"bud\" + 0.236*\"live\" + 0.236*\"learn\" + 0.236*\"year\" + 0.006*\"check\" + 0.006*\"compat\" + 0.006*\"malaysia\" + 0.006*\"sad\" + 0.006*\"photocard\" + 0.006*\"devic\"\n",
      "\n",
      "Score: 0.05000004917383194\t \n",
      "Topic: 0.163*\"phone\" + 0.132*\"note\" + 0.132*\"enhypen\" + 0.132*\"jungwon\" + 0.132*\"member\" + 0.132*\"buy\" + 0.132*\"jungwoni\" + 0.024*\"case\" + 0.004*\"previous\" + 0.003*\"check\"\n",
      "\n",
      "Score: 0.05000001937150955\t \n",
      "Topic: 0.143*\"phone\" + 0.141*\"buy\" + 0.141*\"jungwoni\" + 0.141*\"enhypen\" + 0.141*\"jungwon\" + 0.141*\"note\" + 0.141*\"member\" + 0.001*\"iphon\" + 0.001*\"previous\" + 0.001*\"case\"\n",
      "\n",
      "Score: 0.05000001937150955\t \n",
      "Topic: 0.364*\"iphon\" + 0.204*\"phone\" + 0.056*\"oneplus\" + 0.044*\"case\" + 0.044*\"compat\" + 0.044*\"note\" + 0.037*\"previous\" + 0.025*\"appl\" + 0.019*\"better\" + 0.019*\"hyunsuk\"\n",
      "\n",
      "Score: 0.05000001937150955\t \n",
      "Topic: 0.246*\"previous\" + 0.130*\"iphon\" + 0.117*\"yesterday\" + 0.117*\"mention\" + 0.117*\"hyunsuk\" + 0.117*\"better\" + 0.117*\"vlive\" + 0.011*\"case\" + 0.009*\"phone\" + 0.002*\"bud\"\n",
      "\n",
      "Score: 0.05000001937150955\t \n",
      "Topic: 0.137*\"bud\" + 0.129*\"check\" + 0.119*\"compat\" + 0.119*\"photocard\" + 0.119*\"devic\" + 0.119*\"malaysia\" + 0.119*\"sad\" + 0.013*\"jungwon\" + 0.013*\"jungwoni\" + 0.013*\"enhypen\"\n"
     ]
    }
   ],
   "source": [
    "for index,score in sorted(lda_model[bow_corpus[530]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score,lda_model.print_topic(index,10)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Performance evalution by classifying document using LDA TFIDF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score:0.5499959588050842\t \n",
      "Topic:0.250*\"smartphon\" + 0.131*\"oneplus\" + 0.100*\"appl\" + 0.097*\"check\" + 0.071*\"mobil\" + 0.049*\"realm\" + 0.037*\"iphon\" + 0.027*\"phone\" + 0.022*\"photocard\" + 0.022*\"sad\"\n",
      "\n",
      "Score:0.05000196769833565\t \n",
      "Topic:0.141*\"case\" + 0.113*\"previous\" + 0.108*\"iphon\" + 0.066*\"phone\" + 0.060*\"yesterday\" + 0.060*\"hyunsuk\" + 0.060*\"better\" + 0.060*\"vlive\" + 0.060*\"mention\" + 0.026*\"smartphon\"\n",
      "\n",
      "Score:0.050001151859760284\t \n",
      "Topic:0.149*\"iphon\" + 0.084*\"year\" + 0.084*\"live\" + 0.084*\"learn\" + 0.080*\"note\" + 0.077*\"oneplus\" + 0.067*\"bud\" + 0.047*\"phone\" + 0.042*\"member\" + 0.042*\"buy\"\n",
      "\n",
      "Score:0.0500006377696991\t \n",
      "Topic:0.251*\"year\" + 0.251*\"learn\" + 0.251*\"live\" + 0.194*\"bud\" + 0.009*\"smartphon\" + 0.006*\"oneplus\" + 0.006*\"check\" + 0.006*\"appl\" + 0.001*\"devic\" + 0.001*\"sad\"\n",
      "\n",
      "Score:0.05000009387731552\t \n",
      "Topic:0.141*\"jungwoni\" + 0.141*\"enhypen\" + 0.141*\"buy\" + 0.141*\"member\" + 0.141*\"jungwon\" + 0.137*\"note\" + 0.112*\"phone\" + 0.008*\"live\" + 0.008*\"learn\" + 0.008*\"year\"\n",
      "\n",
      "Score:0.05000003054738045\t \n",
      "Topic:0.156*\"compat\" + 0.124*\"photocard\" + 0.124*\"malaysia\" + 0.124*\"sad\" + 0.124*\"devic\" + 0.113*\"check\" + 0.072*\"bud\" + 0.042*\"iphon\" + 0.018*\"case\" + 0.015*\"learn\"\n",
      "\n",
      "Score:0.05000003054738045\t \n",
      "Topic:0.135*\"previous\" + 0.112*\"iphon\" + 0.108*\"phone\" + 0.062*\"note\" + 0.057*\"enhypen\" + 0.057*\"buy\" + 0.057*\"jungwon\" + 0.057*\"jungwoni\" + 0.057*\"member\" + 0.054*\"yesterday\"\n",
      "\n",
      "Score:0.05000002309679985\t \n",
      "Topic:0.080*\"buy\" + 0.080*\"jungwon\" + 0.080*\"jungwoni\" + 0.080*\"member\" + 0.080*\"enhypen\" + 0.077*\"note\" + 0.066*\"phone\" + 0.037*\"compat\" + 0.033*\"bud\" + 0.031*\"iphon\"\n",
      "\n",
      "Score:0.05000002309679985\t \n",
      "Topic:0.215*\"year\" + 0.215*\"live\" + 0.215*\"learn\" + 0.172*\"bud\" + 0.014*\"devic\" + 0.014*\"malaysia\" + 0.014*\"sad\" + 0.014*\"photocard\" + 0.014*\"compat\" + 0.013*\"check\"\n",
      "\n",
      "Score:0.05000002309679985\t \n",
      "Topic:0.074*\"phone\" + 0.070*\"note\" + 0.067*\"jungwon\" + 0.067*\"member\" + 0.067*\"enhypen\" + 0.067*\"jungwoni\" + 0.067*\"buy\" + 0.042*\"iphon\" + 0.040*\"previous\" + 0.038*\"malaysia\"\n"
     ]
    }
   ],
   "source": [
    "for index,score in sorted(lda_model_tfidf[bow_corpus[530]],key=lambda tup:-1*tup[1]):\n",
    "    print(\"\\nScore:{}\\t \\nTopic:{}\".format(score,lda_model_tfidf.print_topic(index,10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing model on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.45601701736450195\t Topic:0.246*\"previous\" + 0.130*\"iphon\" + 0.117*\"yesterday\" + 0.117*\"mention\" + 0.117*\"hyunsuk\"\n",
      "\n",
      "Score: 0.45509272813796997\t Topic:0.242*\"bud\" + 0.236*\"live\" + 0.236*\"learn\" + 0.236*\"year\" + 0.006*\"check\"\n",
      "\n",
      "Score: 0.011111713945865631\t Topic:0.137*\"bud\" + 0.129*\"check\" + 0.119*\"compat\" + 0.119*\"photocard\" + 0.119*\"devic\"\n",
      "\n",
      "Score: 0.011111676692962646\t Topic:0.364*\"iphon\" + 0.204*\"phone\" + 0.056*\"oneplus\" + 0.044*\"case\" + 0.044*\"compat\"\n",
      "\n",
      "Score: 0.011111223138868809\t Topic:0.192*\"phone\" + 0.073*\"note\" + 0.073*\"jungwon\" + 0.073*\"member\" + 0.073*\"enhypen\"\n",
      "\n",
      "Score: 0.011111140251159668\t Topic:0.163*\"phone\" + 0.132*\"note\" + 0.132*\"enhypen\" + 0.132*\"jungwon\" + 0.132*\"member\"\n",
      "\n",
      "Score: 0.011111137457191944\t Topic:0.134*\"phone\" + 0.129*\"member\" + 0.129*\"jungwoni\" + 0.129*\"note\" + 0.129*\"jungwon\"\n",
      "\n",
      "Score: 0.011111132800579071\t Topic:0.143*\"phone\" + 0.141*\"buy\" + 0.141*\"jungwoni\" + 0.141*\"enhypen\" + 0.141*\"jungwon\"\n",
      "\n",
      "Score: 0.011111126281321049\t Topic:0.426*\"smartphon\" + 0.178*\"oneplus\" + 0.110*\"mobil\" + 0.092*\"appl\" + 0.089*\"realm\"\n",
      "\n",
      "Score: 0.011111126281321049\t Topic:0.130*\"phone\" + 0.121*\"note\" + 0.121*\"buy\" + 0.121*\"jungwoni\" + 0.121*\"member\"\n"
     ]
    }
   ],
   "source": [
    "unseen_doc='Previously Hyunsuk mention in his previous comment as Make their year with Galaxy Buds Live Learn more'\n",
    "bow_vector=dictionary.doc2bow(preprocess(unseen_doc))\n",
    "\n",
    "for index,score in sorted(lda_model[bow_vector],key=lambda tup:-1*tup[-1]):\n",
    "    print(\"\\nScore: {}\\t Topic:{}\".format(score,lda_model.print_topic(index,5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clustering out topics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
